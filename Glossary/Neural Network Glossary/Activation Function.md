- An Activation Function receives the output of the Summation Operator and transforms it into the final output of a node which is a value that represents how much a node should contribute (fire).
![[Pasted image 20230104080459.png|center]]
- The ReLU is the most used activation function in the world right now. Since, it is used in almost all the convolutional neural networks or deep learning.