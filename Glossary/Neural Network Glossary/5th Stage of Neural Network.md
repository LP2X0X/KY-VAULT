- In stage 5, we have learned how the Weights in a Neural Network are updated. At a high level, this is accomplished by moving in the opposite direction of each Gradient (Partial Derivative), calculated in Stage 3. This process is called Gradient Descent due to the fact that the goal is to move down the Gradients to find the lowest error.
- We also learned that there are three popular methods for applying Gradient Descent, including Batch Gradient Descent, Stochastic Gradient Descent, and Mini-batch Gradient Descent. Each varies in the amount of data used to update and the time it takes to update.